{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with TensorFlow and machine learning\n",
    "#### A brief words about machine learning and tensorflow\n",
    "The need for machine learning arises when we have a lot of data and want to learn from it. The data is often given as input and output pairs and we want a computer to find a pattern (we assume there is one) between some input and given outputs. In simple cases, there can be a simple relationship between input and outputs such as the relationship between grams and kilograms which can be described as a linear function. But Machine learning gets more interesting and powerful when the relationship between inputs and outputs is not obvious at first. For example in this tutorial we will learn to map images of digits to their label. Pretty cool, right ?\n",
    "According to the tensorflow website, this is considered the \"hello world\" of machine learning so let's get started. \n",
    "\n",
    "#### Methodology and approach to the problem\n",
    "A machine learning problem can be broken down in four parts : acquiring the data, specifying the models (type of relationship between input and output should look for), training the computer to look for the best model among the set of models he has to explore and evaluating how well the model predicts output on new data.\n",
    "##### The data\n",
    "The first and essential part is the initial data. This data is what the computer will look at to teach himself how to find patterns between input and outputs. In most cases, the data is not given in a clean format and needs to be preprocessed so that the computer knows what to look for (features) and what to predict (outputs). A challenge in machine learning is to extract the features having the most predictive power in the data. <br />\n",
    "This area is called feature engineering and is an important component of machine learning expertise. Recently, a form of machine learning called deep learning reduced the need for extensive human feature engineering allowing to learn complex outputs from raw data. In our cases, the data we will look at is the MNIST data which is made of black-and-white images of number (described by a 28*28 matrix of pixel values) and their labels (the number they represent). A first step is to download this data set in out repo. Thankfully, there is some already written code which does this job for us in the following next three lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading our data, we want to split it into input and output pairs. The simplest way to represent a 28*28 black-and-white image is as a list of 784 pixel values. For the output, each number is described as a one_hot vector (ith element is 1 if the label corresponds to element i). For example 3 will look like : [0,0,0,1,0,0,0,0,0,0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_vector = tf.placeholder(\"float\", [None, 784]) \n",
    "labels = tf.placeholder(\"float\", [None, 10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two previous lines of code, we told TensorFlow that one part of the data will come as a feature vector consisting of floating point number of 784 dimensions and another part of the data will come as a one-hot vector of 10 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model\n",
    "Now after having downloaded our data, we define the type of relationships between image_vectors and labels we want the computer to look for. In our case, the model is called a logistic regression. In logistic regression, we do a weighted sum of each feature in a feature vector and different labels correspond to different weights on each dimensions of the feature vector. This sum is indicative of how much the computer believes that one example belongs to a specific class. The output of this logistic regression model can be seen as a confidence score (more precisely a probability distribution) on each of the different type of label class. For example an output of [0.2,0,0.8,0,0,0,0,0,0,0] indicates a 80% confidence that the image is 2 and 20% confidence that the image is 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Weights = tf.Variable(tf.zeros([784, 10]))\n",
    "bias = tf.Variable(tf.zeros([10]))\n",
    "confidence_score = tf.nn.softmax(tf.matmul(image_vector, Weights) + bias) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training : Choosing the loss function \n",
    "Now after having defined our model, we want to find a way to measure how wrong or how right it is."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
